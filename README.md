# Calculus in Neural Networks  
*A mathematical and computational walkthrough of backpropagation in a simple feedforward network*

## ğŸ“Œ Overview

This project presents a **from-scratch, calculus-based derivation of backpropagation** for a small feedforward neural network, and validates the derivations using Python code.

The work was originally developed as part of my **MSc in Applied Artificial Intelligence & Data Science** and focuses on building intuition for how gradients are computed and used to train neural networks.

The core goals of this project is to demonstrates how calculus enables learning in neural networks by:

    - Deriving backpropagation for a small neural network step-by-step
    - Applying partial derivatives and the chain rule to compute gradients
    - implement and test the weights and biases updates in Python

---

## ğŸ§  Problem & Network Setup

We consider a simple **2â€“2â€“1 feedforward neural network**:

![Network architecture](images/2-2-1NN.png)

- **Input layer:** 2 features  
- **Hidden layer:** 2 neurons (with activation function, e.g., sigmoid)  
- **Output layer:** 1 neuron (for regression or binary classification)  

The project walks through:

1. Forward pass equations in matrix form  
2. Loss function definition (e.g., mean squared error or cross-entropy)  
3. Manual derivation of:
   - Gradients of the loss with respect to weights and biases
   - Chain rule applications layer by layer
4. Backward pass formulas for parameter updates

---

## ğŸ“ Mathematics Covered

The report and code discuss:
- Application of the **chain rule** in multi-layer networks  
- Gradients of:
  - Linear layers (`z = Wx + b`)  
  - Activation functions (e.g., sigmoid derivative)  
  - Loss with respect to outputs and intermediate variables  
- How local gradients compose to form global gradients across the network 

![Backpropagation in Neural Network](images/Backpropagation.png)

---

## ğŸ—‚ï¸ Repository Structure

```bash
calculus-neural-networks/
â”œâ”€â”€ README.md                     # Project overview (this file)
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ Calculus_Behind_Neural_Networkss.pdf   # Final project 
â”‚   â”œâ”€â”€ Calculus-Behind-Neural-Networkss.pdf   # Presentation
â””â”€â”€ src/
â”‚   â”œâ”€â”€ manual_NN.ipynb             
â”‚   â””â”€â”€ sklearn_NN.ipynb
â”‚   
â””â”€â”€ images/ 
       â”œâ”€â”€ 2-2-1.png             
       â””â”€â”€ Backpropagation.png


## ğŸ“„ Project Documents

This repository includes both the full project report and the final presentation slides:

- **Report:** `report/Calculus_Behind_Neural_Networks.pdf`
- **Presentation:** `report/Calculus-Behind-Neural-Networks.pdf`

## ğŸ“Š Summary of Findings

Results
â€¢ Successfully computed all gradients for a real network example
â€¢ Performed one full weight-update step using gradient descent
â€¢ Confirmed that parameter updates follow gradient direction

Conclusion
â€¢ Chain rule enables backpropagation.
â€¢ Partial derivatives show how each parameter affects loss.
â€¢ Without calculus, neural networks cannot learn, calculus is the learning engine.
